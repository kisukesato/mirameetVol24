{"./":{"url":"./","title":"はじめに","keywords":"","body":"GCP初心者必見！ PythonでBigQueryの操作をしてみよう！ 第24回目の開催となるmirameetの題材はGCP初心者必見★今回のハンズオンはGoogle Cloud Storageの開発Tipsも取り込んだ内容になっています！ 事前準備 Dockerインストール GitHubアカウントの作成 GCPアカウントの作成 今回の流れ CSVファイルをGCS（GoogleCloudStroage）上にアップロード、CSVファイルのデータをBigQueryに投入、BigQuery内のデータを確認するバッチを作成し実行します。 技術要素 公式サイトのリンクを記載しておきますので、参考にお使いください。 GCP Docker python 手順 全体手順としては次の流れで進めます。 0.事前準備内容の確認 1.GCP各種サービスの設定 2.ダウンロードしたファイルの解凍と設定 3.各処理の実行と処理結果の確認 Windows/Macの方向けに作成しております。コマンドラインツールは、個々の利用しているもので良いのですが、今回の手順は次のものを利用します。 Windows：コマンドプロンプト Mac：ターミナル "},"Section1.html":{"url":"Section1.html","title":"0. 事前準備内容の確認","keywords":"","body":"事前準備内容の確認 GCPアカウントの確認 ログインすると下記のようなダッシュボード画面が表示されます。 Dockerの動作確認 試しに下記のコマンドを実行すると、docker-composeのバージョンが確認できます。 docker-compose version ソースファイルの取得 画面右上の「Code」から「Download ZIP」を選択ダウンロードしたファイルは後程使用します。 "},"Section2.html":{"url":"Section2.html","title":"1. GCP各種サービスの設定","keywords":"","body":"GCP各種サービスの設定 この手順では、GCPサービスの設定を進めていきます。 GCSバケット作成 左上のハンバーガーメニューから、CloudStorageを選択 「バケットを作成」をクリック グローバルに一意になるように各自設定を行います。 ↓作成完了 サービスアカウントの作成 左上のハンバーガーメニューから、IAMと権利＞サービスアカウントを選択 任意のサービスアカウント名を入力 ↓作成完了 秘密鍵ファイルの取得・配置 作成したサービスアカウントを選択 「キー」タブ＞「新しい鍵を作成」を選択 json形式のキーをダウンロード ↓ダウンロード完了 CloudStorageAPIが有効化されていることを確認 画面上部の検索窓に「Cloud Storage」と入力し、検索結果から「Cloud Storage API」を選択 ※「APIが有効です」となっていることを確認 BigQueryAPIが有効化されていることを確認 画面上部の検索窓に「BigQuery API」と入力し、検索結果から「BigQuery API」を選択 ※「APIが有効です」となっていることを確認 データセットの作成 左上のハンバーガーメニューから、BigQueryを選択 エクスプローラーの中の「▶プロジェクト名」から「データセットを作成」を選択 画面右側に「データセットを作成する」が出てくるので、「データセットID」を入力※今回は「mira_vol24」を指定 ↓完了「▶プロジェクト名」の下に「▶mira_vol24」が作成される テーブルの作成 作成したデータセット「mira_vol24」に対して、クエリを実行し、テーブルを作成※今回は「mira_example」を指定 下記のクエリは、 データセットID＝mira_vol24 テーブルID＝mira_example を指定しています CREATE TABLE mira_vol24.mira_example ( id NUMERIC, mira_code STRING, mira_text STRING, work_date STRING ) ※データセットIDを「mira_vol24」から変更した場合は、　ご自身が指定したデータセットIDに置換して実行する必要があります※先ほどDLしたソースファイルの\\sql\\mira_vol24.sql の中にも同じSQLがあります↓実行完了 先ほど実行したクエリ通りのフィールドが表示されていることを確認 "},"Section3.html":{"url":"Section3.html","title":"2. ダウンロードしたファイルの解凍と設定","keywords":"","body":"ダウンロードしたファイルの解凍と設定 この手順では、GithubからダウンロードしたZipファイルを任意のディレクトリで解凍し、コーディングしていきます。 OperationObject.pyの編集 # ------------------------------------------------------ # 変数 # ------------------------------------------------------ GOOGLE_APPLICATION_CREDENTIALS='./credential/key.json' # CSVコピー元 source_file_name = './csv/gcs-example.csv' url_gs_example_csv=\"gs://mira-example/gcs-example.csv\" # 要変更1.1 out_url_gs_example_csv=\"gs://mira-example/out-gcs-example.csv\" # 要変更1.1 # GS バケット bucket_name = \"mira-example\" # 要変更1.2 # 操作テーブル project_id = \"erudite-pride-323410\" # 要変更1.3 dataset_id = \"mira_vol24\" # 任意で変更1.4 table_id = \"mira_example\" # 任意で変更1.5 gsutil URI Cloud Storageのバケットの情報から「gsutil URI」をコピーして置換 （例） url_gs_example_csv=\"gs://mira-example/gcs-example.csv\" out_url_gs_example_csv=\"gs://mira-example/out-gcs-example.csv\" ↓ url_gs_example_csv=\"gs://mirameet24_test00/gcs-example.csv\" out_url_gs_example_csv=\"gs://mirameet24_test00/out-gcs-example.csv\" bucket_name グローバルに一意になるように作成したバケット名をコピーして置換 （例） bucket_name = \"mira-example\" ↓ bucket_name = \"mirameet24_test00\" project_id GCPダッシュボードの左上から「プロジェクトID」をコピーして置換 （例） project_id = \"erudite-pride-323410\" ↓ project_id = \"neural-sol-325512\" dataset_id 「データセットID」を変更した場合、任意で設定した値に置換 dataset_id = \"mira_vol24\" table_id テーブル作成時のクエリから変更した場合、任意で設定した値に置換 table_id = \"mira-example\" ダウンロードしたjson形式keyファイルの配置 ・ファイル名を下記に変更 key.json ・解凍したZipファイルのcredentialに配置 ～～～mirameetVol24-main\\src\\credential "},"Section4.html":{"url":"Section4.html","title":"3. 各処理の実行と処理結果の確認","keywords":"","body":"各処理の実行と処理結果の確認 この手順では、コーディングしたソースを動かし、BigQueryのデータを操作していきます。 カレントディレクトリの移動 Zipファイルを解凍したディレクトリまで移動 cd ～～～mirameetVol24-main Dockerでコンテナを起動 docker-compose up -d --build カレントディレクトリの移動 srcディレクトリまで移動 cd ～～～mirameetVol24-main\\src GcsUploader01.pyの実行 ★★★画像差し込み★★★CSVデータファイルをGCSバケット上にアップロードする python GcsUploader01.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import storage # GCS認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # GCSクライアント変数宣言 client = storage.Client() # GCSバケット取得 bucket = client.get_bucket(OperationObject.bucket_name) # CSVファイルアップロード blob = bucket.blob(os.path.basename(OperationObject.source_file_name)) blob.upload_from_filename(OperationObject.source_file_name) print('File {} uploaded to {}.'.format( OperationObject.source_file_name, bucket)) ↓正常終了mirameetVol24-main\\src\\csvの中の「gcs-example.csv」が、GCSにアップロードされていることを確認 GcsToBigQuery02.pyの実行 ★★★画像差し込み★★★GCSにアップロードしたCSVデータをBigQueryにインサートする python GcsToBigQuery02.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCS認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # テーブルIDの取得 client = bigquery.Client(OperationObject.project_id) table_id = client.dataset(OperationObject.dataset_id).table(OperationObject.table_id) # テーブル設定宣言 job_config = bigquery.LoadJobConfig( schema=[ bigquery.SchemaField(\"id\", \"NUMERIC\"), bigquery.SchemaField(\"mira_code\", \"STRING\"), bigquery.SchemaField(\"mira_text\", \"STRING\"), bigquery.SchemaField(\"work_date\", \"STRING\") ], skip_leading_rows=0, # The source format defaults to CSV, so the line below is optional. source_format=bigquery.SourceFormat.CSV, ) # 実行前にテーブルをトランケート job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE # GSバケットをロードし、テーブルに登録 load_job = client.load_table_from_uri( OperationObject.url_gs_example_csv, table_id, job_config=job_config ) # Make an API request. load_job.result() # Waits for the job to complete. destination_table = client.get_table(table_id) # Make an API request. print(\"Loaded {} rows.\".format(destination_table.num_rows)) ↓正常終了「gcs-example.csv」の中のデータがBigQueryにインサートされていることを確認プレビュータグでデータの中身を確認 UpdateDeleteBigQuery03.pyの実行 ★★★画像差し込み★★★BigQueryのデータを更新・削除する python UpdateDeleteBigQuery03.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCS認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS client = bigquery.Client() # 更新SQL生成 updateQuery = \"UPDATE `{0}.{1}.{2}` SET mira_text = '更新' WHERE id = 2\".\\ format(OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id) # SQL実行 updateRows = client.query(updateQuery).result() print(\"Updated ID=2.\") # 削除SQL生成 deleteQuery = \"DELETE `{0}.{1}.{2}` WHERE id = 3\".\\ format(OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id) # SQL実行 deleteRows = client.query(deleteQuery).result() print(\"deleted ID=3.\") ↓正常終了ID＝2のデータが更新、ID＝3のデータが削除されていることを確認 ExportBigQuery04.pyの実行 ★★★画像差し込み★★★BigQueryのデータをCSVデータファイルとしてGCSバケットにエクスポートする python ExportBigQuery04.py ▼中身 import os import OperationObject # 操作対象の設定情報取得 from google.cloud import bigquery # GCS認証設定 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = OperationObject.GOOGLE_APPLICATION_CREDENTIALS # テーブルIDの取得 client = bigquery.Client(OperationObject.project_id) table_id = client.dataset(OperationObject.dataset_id).table(OperationObject.table_id) extract_job = client.extract_table( table_id, OperationObject.out_url_gs_example_csv, ) # API request extract_job.result() # Waits for job to complete. print( \"Exported {}:{}.{} to {}\".format( OperationObject.project_id, OperationObject.dataset_id, OperationObject.table_id, OperationObject.out_url_gs_example_csv) ) ↓正常終了BigQueryのデータが「out-gcs-example.csv」ファイルとしてGCSに出力されていることを確認ダウンロードボタンでファイルをローカルにダウンロードダウンロードしたCSVファイルの中身で更新後の状態を確認 "}}